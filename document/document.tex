\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[parfill]{parskip}

\title{Natural Language Processing: Sentiment Analysis}
\author{FangRui}
\date{November 2021}

\begin{document}
\maketitle
	\begin{abstract}
		This work is a simple implementation of a deep learning model consisting of three components: \textit{Word Embedding}, \textit{Encoder} and \textit{Classifier}. The model is based on a \textbf{pytorch} implementation. Different word embedding methods: \textit{Random Initialisation,Word2Vec, GloVe} and different encoders: \textit{RNN, LSTM, GRU} -- are tried in the model during the experiments.
		
		The report is divided into five main sections, followed by a description of the main processing modules used in the experiments(\textbf{Section \uppercase\expandafter{\romannumeral1}}), the dataset(\textbf{Section \uppercase\expandafter{\romannumeral2}}), the extra module and training strategies(\textbf{Section \uppercase\expandafter{\romannumeral3}}), modules comparison (\textbf{Section \uppercase\expandafter{\romannumeral4}}), and the tips gained from the experiments(\textbf{Section \uppercase\expandafter{\romannumeral5}}).
	\end{abstract}

	\section{Introduction}
	\subsection{Word embedding}
	\subsubsection{Random Initialization}
	\textit{Random Initialization} randomly initializes a simple lookup table that stores embeddings of a fixed dictionary and size.
	\subsubsection{Word2Vector}
	\textit{Word2vec} algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level of semantic similarity between the words represented by those vectors.
	\subsubsection{GloVe}
	\textit{GloVe} is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.
	
	\subsection{Encoder}
	\subsubsection{RNN}
	\textit{Recurrent neural network (RNN)} is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs.
	\subsubsection{LSTM}
	\textit{Long short-term memory (LSTM)} is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.
	A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.
	
	LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden Markov models and other sequence learning methods in numerous applications.
	\subsubsection{GRU}
	\textit{GRU} is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. GRU’s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.
	
	\section{Dataset}
	
	The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.
	
	Each phrase is labelled as either negative, somewhat negative, neutral, somewhat positive or positive. The corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary.
	
	\section{Extra module and Training Strategy}
	\subsection{Attention}
	\textit{Attention} is a technique that mimics cognitive attention. The effect enhances the important parts of the input data and fades out the rest—the thought being that the network should devote more computing power to that small but important part of the data. Which part of the data is more important than others depends on the context and is learned through training data by gradient descent.
	\subsection{Training strategy}
	Learning rate decay and gradient clipping are used for better training and prevent overfitting
	
	\section{Comparison}
	Table \ref{table:data} demonstrate the accuracy of different models on validation dataset of SST-2.
	
	\begin{table}[h!]
		\centering
		\begin{tabular}{||c | c||} 
			\hline
			& \\
			Models & SST-2 Accuracy \\
			& \\
			\hline\hline
			& \\
			Random-BiLSTM & 70.06 \\ 
			GloVe-BiLSTM & 78.13\\
			Word2Vec-BiLSTM & 77.52 \\
			GloVe-GRU-Attn & 83.01 \\
			GloVe-LSTM-Attn & \textbf{84.16} \\
			& \\
			\hline
		\end{tabular}
		\caption{Validation accuracy on different models}
		\label{table:data}
	\end{table}
	
	\section{Tips}
	We use batch size = 16, learning rate = $10^{-4}$, and weight decaying = $10^{-5}$. For learning rate decaying, we made learning rate update as: $LR' = 0.5^{\frac{epoch - threshold}{50}} * LR$, where threshold is the epoch starting learning rate decay.
	
\end{document}